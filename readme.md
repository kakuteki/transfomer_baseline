# 独英翻訳Transformerモデル

PyTorchで実装された1層Transformerモデルによる独→英翻訳システムです。Multi30kデータセットを使用して訓練されます。

## 特徴

- **軽量な1層Transformer**: エンコーダー・デコーダーそれぞれ1層の軽量設計
- **Multi30kデータセット**: 約29,000の独英翻訳ペアを使用
- **マルチヘッドアテンション**: 8ヘッドのセルフアテンション機構
- **ビームサーチ対応**: グリーディサーチに加えてビームサーチにも対応
- **インタラクティブ翻訳**: 訓練後に対話的に翻訳を実行可能

## 必要な依存関係

```bash
pip install -r requirements.txt
```

### spaCyモデルのインストール

```bash
python -m spacy download de_core_news_sm
python -m spacy download en_core_web_sm
```

## 使用方法

### 1. データセットのダウンロード

```bash
python download_data.py
```

### 2. モデルの訓練

```bash
python app.py
```

### 3. インタラクティブ翻訳

```bash
python app.py interactive
```

## ファイル構成

- `app.py`: メインの訓練・評価スクリプト
- `download_data.py`: Multi30kデータセットのダウンロード
- `data/`: データセットの保存ディレクトリ
- `best_model.pt`: 訓練された最良モデル
- `requirements.txt`: 必要なライブラリ一覧

## モデル構成

### ハイパーパラメータ

- **d_model**: 256 (隠れ層の次元数)
- **n_heads**: 8 (マルチヘッドアテンションのヘッド数)
- **n_encoder_layers**: 1 (エンコーダー層数)
- **n_decoder_layers**: 1 (デコーダー層数)
- **d_ff**: 1024 (フィードフォワード層の次元数)
- **dropout**: 0.1
- **batch_size**: 64
- **num_epochs**: 30
- **learning_rate**: 1e-3 (warmupスケジュール付き)

### アーキテクチャ

1. **エンベディング層**: 語彙を256次元ベクトルに変換
2. **位置エンコーディング**: 正弦波による位置情報付加
3. **エンコーダー**: 1層のマルチヘッドアテンション + フィードフォワード
4. **デコーダー**: 1層のマスクアテンション + クロスアテンション + フィードフォワード
5. **出力層**: 語彙サイズの線形層

## 評価指標

- **Loss**: クロスエントロピー損失
- **BLEU**: sacrebleuによるBLEUスコア計算
- グリーディサーチとビームサーチ（beam_size=3）の両方で評価

## 注意事項

- GPUが利用可能な場合は自動的にCUDAを使用します
- メモリ使用量を抑えるため、1層の軽量モデル設計となっています
- 訓練には約30エポック、数時間程度かかります

## ライセンス

このプロジェクトは教育目的で作成されています。